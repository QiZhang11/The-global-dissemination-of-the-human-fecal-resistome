import pandas as pd
import numpy as np
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn import preprocessing
'''
name=['T_xacc', 'T_yacc',' T_zacc',' T_xgyro', ' T_ygyro',' T_zgyro','T_xmag','T_ymag', 'T_zmag',
      'RA_xacc','RA_yacc',' RA_zacc',' RA_xgyro', ' RA_ygyro',' RA_zgyro','RA_xmag','RA_ymag', 'RA_zmag,',
      'LA_xacc', 'LA_yacc',' LA_zacc',' LA_xgyro', ' LA_ygyro',' LA_zgyro','LA_xmag','LA_ymag', 'LA_zmag',
      'RL_xacc', 'RL_yacc',' RL_zacc',' RL_xgyro', ' RL_ygyro',' RL_zgyro','RL_xmag','RL_ymag', 'RL_zmag',
      'LL_xacc', 'LL_yacc',' LL_zacc',' LL_xgyro', ' LL_ygyro',' LL_zgyro','LL_xmag','LL_ymag', 'LL_zmag']
folder='data'
X=[]
Y=[]
i=0
for path_a in os.listdir(folder):
      i+=1
      for path_b in os.listdir(os.path.join(folder,path_a)):
            for path_c in os.listdir(os.path.join(folder,path_a,path_b)):
                  data = pd.read_csv(os.path.join(folder, path_a, path_b,path_c))
                  Data=np.array(data).reshape(-1)
                  X.append(Data)
                  Y.append(i)
x=np.array(X)
y=np.array(Y)
#sample=np.vstack((x,y))
#sample=np.concatenate((x,y),axis=0)
sample=np.c_[x,y]
sample=np.random.shuffle(sample)
#Sample=pd.DataFrame(sample)
#print(x.shape)
#print(y.shape)
#print(sample.shape)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0, train_size=0.8)
forest=RandomForestClassifier()
forest.fit(x_train,y_train.astype('int'))
forest_score=forest.score(x_test,y_test.astype('int'))
y_pred=forest.predict(x_test)
print('score',forest_score)
#Sample.to_csv('data1.csv')
'''
Date=pd.read_csv('gut AMR-cnn.csv')
Date=np.array(Date)[:,1:]
np.random.shuffle(Date)

sample=Date
print(sample.shape)

x=np.array(sample[:,:-1],dtype=float)
minmax_scale = preprocessing.MinMaxScaler()
x = minmax_scale.fit_transform(x)
y=np.array(sample[:,-1],dtype=float)

encoder = OneHotEncoder()
encoder.fit(np.arange(1,4).reshape(-1, 1))
def one_hot_encode(x):
    return encoder.transform(np.array(x).reshape(-1, 1)).toarray()
y_data = one_hot_encode(y)
x_train, x_test, y_train, y_test = train_test_split(x, y_data, random_state=0, train_size=0.8)
model=tf.keras.models.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1,x_train.shape[1]]))
model.add(tf.keras.layers.Dense(5000,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(3000,activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(2000,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(1500,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(1000,activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(800,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(500,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(300,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(150,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(80,activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(40,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(20,activation="relu"))
#model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(10,activation="relu"))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(3,activation="softmax"))
model.summary()

model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001),metrics=["accuracy"])
history=model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test))
#history=model.fit(x,y_data,epochs=60,validation_data=(x,y_data))
model.save('modelx.h5')
plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['loss','val_loss'],loc='upper right')
plt.savefig('loss_all_x.png')
plt.show()
plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['accuracy','val_accuracy'],loc='upper right')
plt.savefig('accuracy_all_x.png')
plt.show()

y_pred = model.predict(x_test)
B = tf.argmax(y_pred,1)
A = tf.argmax(y_test,1)

Matrix = tf.math.confusion_matrix(
    A, B, num_classes=3, weights=None, dtype=tf.dtypes.float32,
    name=None
)

# matrix_sum =  np.sum(Matrix,axis=1)
# matrix = Matrix/matrix_sum
plt.figure(figsize=(15,15))
sn.heatmap(Matrix, annot=True, fmt=".2f",cmap='RdPu')
plt.savefig('testx.tif',dpi=600)
plt.show()

#data = pd.read_csv('s01.txt',header=None,names=name)
